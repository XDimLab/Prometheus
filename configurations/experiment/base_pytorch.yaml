# inherites from base_experiment.yaml
# most of the options have docs at https://lightning.ai/docs/pytorch/stable/common/trainer.html

defaults:
  - base_experiment

tasks: [training] # tasks to run sequantially, change when your project has multiple stages and you want to run only a subset of them.
num_nodes: 1 # number of gpu servers used in large scale distributed training

training:
  gpus: ${GPUS}
  check_val_every_n_epoch: 1
  gradient_clip_val: 0.01
  resume_from_director3d: ${PRETRAINED_PATH}/model.ckpt
  lpips_model_path: null
  gsdecoder_path: ${PRETRAINED_PATH}/gsdecoder/epoch=383-step=383999.ckpt
  depth_model_path: ${HF_PATH}/depth-anything/Depth-Anything-V2-Small-hf
  sd_model_key: ${HF_PATH}/stabilityai/stable-diffusion-2-1-base
  unet_pretrained_path: ${HF_PATH}/stabilityai/stable-diffusion-2-1-base/v2-1_512-ema-pruned.ckpt
  resume_from_checkpoint: ~
  resume_weights_only: False

  use_deepspeed: False
  precision: bf16-mixed # set float precision, 16-mixed is faster while 32 is more stable
  render_bg_color: random
  compile: False # whether to compile the model with torch.compile
  lr: 0.001 # learning rate
  batch_size: 16 # training batch size; effective batch size is this number * gpu * nodes iff using distributed training
  #max_steps: 1000000
  single_view_num: 1 # num of 2d data per iteratiion
  log_every_n_step: 1000
  max_epochs: 1000 # set to -1 to train forever
  max_steps: -1 # set to -1 to train forever, will override max_epochs
  steps_per_epoch: -1
  max_time: null # set to something like "00:12:00:00" to enable

  num_workers: 12 # number of CPU threads for data preprocessing.
  shuffle: True # whether training data will be shuffled
  optim:
    accumulate_grad_batches: 1 # accumulate gradients for n batches before backprop
    # gradient_clip_val: 1. # clip gradients with norm above this value, set to 0 to disable
  checkpointing:
    # these are arguments to pytorch lightning's callback, `ModelCheckpoint` class
    every_n_train_steps: 5000 # save a checkpoint every n train steps
    every_n_epochs: null # mutually exclusive with ``every_n_train_steps`` and ``train_time_interval``
    train_time_interval: null # in format of "00:12:00:00", mutually exclusive with ``every_n_train_steps`` and ``every_n_epochs``.
    enable_version_counter: False # If this is ``False``, later checkpoint will be overwrite previous ones.

validation:
  precision: bf16-mixed
  compile: False # whether to compile the model with torch.compile
  batch_size: 16 # validation batch size per GPU; effective batch size is this number * gpu * nodes iff using distributed training
  val_every_n_step: 2000 # controls how frequent do we run validation, can be float (fraction of epoches) or int (steps) or null (if val_every_n_epoch is set)
  val_every_n_epoch: null # if you want to do validation every n epoches, requires val_every_n_step to be null.
  limit_batch: null # if null, run through validation set. Otherwise limit the number of batches to use for validation.
  inference_mode: True # whether to run validation in inference mode (enable_grad won't work!)

  num_workers: 12 # number of CPU threads for data preprocessing, for validation.
  shuffle: False # whether validation data will be shuffled

test:
  precision: 16-mixed
  compile: False # whether to compile the model with torch.compile
  batch_size: 16 # test batch size per GPU; effective batch size is this number * gpu * nodes iff using distributed training
  limit_batch: null # if null, run through test set. Otherwise limit the number of batches to use for test.
  data:
    num_workers: 16 # number of CPU threads for data preprocessing, for test.
    shuffle: False # whether test data will be shuffled
